{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec18a04",
   "metadata": {},
   "source": [
    "# Lab -- Dataset Curation with Multiple Annotators\n",
    "\n",
    "Intended to accompany the lecture on Dataset Creation and Curation, this notebook contains exercises to analyze an existing classification dataset labeled by multiple annotators (e.g. collected via crowdsourcing).\n",
    "\n",
    "You may find it helpful to first install the following dependencies:\n",
    "\n",
    "**Installed in venv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07588b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cleanlab\n",
    "# # We originally used the version: cleanlab==2.2.0\n",
    "# # This automatically installs other required packages like numpy, pandas, sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c2885ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04a118",
   "metadata": {},
   "source": [
    "## Analyzing dataset labeled by multiple annotators\n",
    "\n",
    "We simulate a small classification dataset (3 classes, 2-dimensional features) with ground truth labels that are then hidden from our analysis. The analysis is conducted on labels from noisy annotators whose labels are derived from the ground truth labels, but with some probability of error in each annotated label where the probability is determined by the underlying quality of the annotator. In subsequent exercises, you should assume the ground truth labels and the true annotator qualities are unknown to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece750ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You don't need to understand this cell, it's just used for generating the dataset\n",
    "\n",
    "SEED = 123  # for reproducibility\n",
    "np.random.seed(seed=SEED)\n",
    "\n",
    "def make_data(sample_size = 300):\n",
    "    \"\"\" Produce a 3-class classification dataset with 2-dimensional features and multiple noisy annotations per example. \"\"\"\n",
    "    num_annotators=50  # total number of data annotators\n",
    "    class_frequencies = [0.5, 0.25, 0.25]\n",
    "    sizes=[int(np.ceil(freq*sample_size)) for freq in class_frequencies]  # number of examples belonging to each class\n",
    "    good_annotator_quality = 0.6\n",
    "    bad_annotator_quality = 0.3\n",
    "    \n",
    "    # Underlying statistics of the datset (unknown to you)\n",
    "    means=[[3, 2], [7, 7], [0, 8]]\n",
    "    covs=[[[5, -1.5], [-1.5, 1]], [[1, 0.5], [0.5, 4]], [[5, 1], [1, 5]]]\n",
    "    \n",
    "    m = len(means)  # number of classes\n",
    "    n = sum(sizes)\n",
    "    local_data = []\n",
    "    labels = []\n",
    "\n",
    "    # Generate features and true labels\n",
    "    for idx in range(m):\n",
    "        local_data.append(\n",
    "            np.random.multivariate_normal(mean=means[idx], cov=covs[idx], size=sizes[idx])\n",
    "        )\n",
    "        labels.append(np.array([idx for i in range(sizes[idx])]))\n",
    "    X_train = np.vstack(local_data)\n",
    "    true_labels_train = np.hstack(labels)\n",
    "\n",
    "    # Generate noisy labels from each annotator\n",
    "    s = pd.DataFrame(\n",
    "        np.vstack(\n",
    "            [\n",
    "                generate_noisy_labels(true_labels_train, good_annotator_quality)\n",
    "                if i < num_annotators - 10  # last 10 annotators are worse\n",
    "                else generate_noisy_labels(true_labels_train, bad_annotator_quality)\n",
    "                for i in range(num_annotators)\n",
    "            ]\n",
    "        ).transpose()\n",
    "    )\n",
    "\n",
    "    # Each annotator only labels approximately 10% of the dataset (unlabeled points represented with NaN)\n",
    "    s = s.apply(lambda x: x.mask(np.random.random(n) < 0.9)).astype(\"Int64\")\n",
    "    s.dropna(axis=1, how=\"all\", inplace=True)\n",
    "    s.columns = [\"A\" + str(i).zfill(4) for i in range(1, num_annotators+1)]\n",
    "    # Drop rows not annotated by anybody\n",
    "    row_NA_check = pd.notna(s).any(axis=1)\n",
    "    X_train = X_train[row_NA_check]\n",
    "    true_labels_train = true_labels_train[row_NA_check]\n",
    "    multiannotator_labels = s[row_NA_check].reset_index(drop=True)\n",
    "    # Shuffle the rows of the dataset\n",
    "    shuffled_indices = np.random.permutation(len(X_train))\n",
    "    return {\n",
    "        \"X_train\": X_train[shuffled_indices],\n",
    "        \"true_labels_train\": true_labels_train[shuffled_indices],\n",
    "        \"multiannotator_labels\": multiannotator_labels.iloc[shuffled_indices],\n",
    "    }\n",
    "\n",
    "def generate_noisy_labels(true_labels, annotator_quality):\n",
    "    \"\"\" Randomly flips each true label to a different class with probability that depends on annotator_quality. \"\"\"\n",
    "    n = len(true_labels)\n",
    "    m = np.max(true_labels) + 1  # number of classes\n",
    "    annotated_labels = np.random.randint(low=0, high=3, size=n)\n",
    "    correctly_labeled_indices = np.random.random(n) < annotator_quality\n",
    "    annotated_labels[correctly_labeled_indices] = true_labels[correctly_labeled_indices]\n",
    "    return annotated_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9e83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = make_data(sample_size = 300)\n",
    "\n",
    "X = data_dict[\"X_train\"]\n",
    "multiannotator_labels = data_dict[\"multiannotator_labels\"]\n",
    "true_labels = data_dict[\"true_labels_train\"] # used for comparing the accuracy of consensus labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21412429",
   "metadata": {},
   "source": [
    "Let's view the first few rows of the data used for this exercise. Here are the labels selected by each annotator for the first few examples. Here each example is a row, and the annotators are columns. Not all annotators labeled each example; valid class annotations from those that did label the example are integers (either 0, 1, or 2 for our 3 classes), and otherwise the annotation is left as `NA` if a particular annotator did not label a particular example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa80889d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A0001</th>\n",
       "      <th>A0002</th>\n",
       "      <th>A0003</th>\n",
       "      <th>A0004</th>\n",
       "      <th>A0005</th>\n",
       "      <th>A0006</th>\n",
       "      <th>A0007</th>\n",
       "      <th>A0008</th>\n",
       "      <th>A0009</th>\n",
       "      <th>A0010</th>\n",
       "      <th>...</th>\n",
       "      <th>A0041</th>\n",
       "      <th>A0042</th>\n",
       "      <th>A0043</th>\n",
       "      <th>A0044</th>\n",
       "      <th>A0045</th>\n",
       "      <th>A0046</th>\n",
       "      <th>A0047</th>\n",
       "      <th>A0048</th>\n",
       "      <th>A0049</th>\n",
       "      <th>A0050</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     A0001  A0002  A0003  A0004  A0005  A0006  A0007  A0008  A0009  A0010  \\\n",
       "247   <NA>      2   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   \n",
       "290   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   \n",
       "262   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>      2   <NA>   <NA>   <NA>   \n",
       "182   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   \n",
       "143   <NA>   <NA>      0   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   \n",
       "\n",
       "     ...  A0041  A0042  A0043  A0044  A0045  A0046  A0047  A0048  A0049  A0050  \n",
       "247  ...   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n",
       "290  ...   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n",
       "262  ...   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n",
       "182  ...   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n",
       "143  ...   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiannotator_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2058d6fb",
   "metadata": {},
   "source": [
    "Here are the corresponding 2D data features for these examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a59e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.01592896, 10.62213634],\n",
       "       [-1.91393643,  6.53944268],\n",
       "       [ 0.55962291,  5.35885902],\n",
       "       [ 6.73677377,  5.02311322],\n",
       "       [ 6.95949986,  1.61434817]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "782aa883-c027-4199-8345-021c32db67d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f836e",
   "metadata": {},
   "source": [
    "### Train model with cross-validation\n",
    "\n",
    "In this exercise, we consider the simple K Nearest Neighbors classification model, which produces predicted class probabilities for a particular example via a (weighted) average of the labels of the K closest examples. We will train this model via cross-validation and use it to produce held-out predictions for each example in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77527d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "def train_model(labels_to_fit):\n",
    "    \"\"\" Trains a simple feedforward neural network model on the data features X with y = labels_to_fit, via cross-validation.\n",
    "        Returns out-of-sample predicted class probabilities for each example in the dataset\n",
    "        (from a copy of model that was never trained on this example).\n",
    "        Also evaluates the held-out class predictions against ground truth labels.\n",
    "    \"\"\"\n",
    "    num_crossval_folds = 5  # number of folds of cross-validation\n",
    "    # model = MLPClassifier(max_iter=1000, random_state=SEED)\n",
    "    model = KNeighborsClassifier(weights=\"distance\")\n",
    "    pred_probs = cross_val_predict(\n",
    "        estimator=model, X=X, y=labels_to_fit, cv=num_crossval_folds, method=\"predict_proba\"\n",
    "    )\n",
    "    class_predictions = np.argmax(pred_probs, axis=1)\n",
    "    held_out_accuracy = np.mean(class_predictions == true_labels)\n",
    "    print(f\"Accuracy of held-out model predictions against ground truth labels: {held_out_accuracy}\")\n",
    "    return pred_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4630fa59",
   "metadata": {},
   "source": [
    "Here we demonstrate how to train and evaluate this model. Note that the evaluation is against ground truth labels, which you wouldn't have in real applications, so this evaluation is just for demonstration purposes. We'll first fit this model using labels comprised of one randomly selected annotation for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d6cde68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random annotators' labels against ground truth labels: 0.6822742474916388\n",
      "Accuracy of held-out model predictions against ground truth labels: 0.8093645484949833\n"
     ]
    }
   ],
   "source": [
    "labels_from_random_annotators = true_labels.copy()\n",
    "for i in range(len(multiannotator_labels)):\n",
    "    annotations_for_example_i = multiannotator_labels.iloc[i][pd.notna(multiannotator_labels.iloc[i])]\n",
    "    labels_from_random_annotators[i] = np.random.choice(annotations_for_example_i.values)\n",
    "\n",
    "print(f\"Accuracy of random annotators' labels against ground truth labels: {np.mean(labels_from_random_annotators == true_labels)}\")\n",
    "pred_probs_from_model_fit_to_random_annotators = train_model(labels_to_fit = labels_from_random_annotators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60403d0",
   "metadata": {},
   "source": [
    "We can also fit this model using the ground truth labels (which you would not be able to in practice), just to see how good it could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee1a3d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of held-out model predictions against ground truth labels: 0.9765886287625418\n"
     ]
    }
   ],
   "source": [
    "pred_probs_from_unrealistic_model_fit_to_true_labels = train_model(labels_to_fit = true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b36c4",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Compute majority-vote consensus labels for each example from the data in `multiannotator_labels`. Think about how to best break ties!\n",
    "\n",
    "- Evaluate the accuracy of these majority-vote consensus labels against the ground truth labels.\n",
    "- Also set these as `labels_to_fit` in `train_model()` to see the resulting model's accuracy when trained with majority vote consensus labels.\n",
    "- Estimate the quality of annotator (how accurate their labels tend to be overall) using only these majority-vote consensus labels (assume the ground truth labels are unavailable as they would be in practice). Who do you guess are the worst 10 annotators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ee57297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of majority vote annotators' labels against ground truth labels: 0.882943143812709\n",
      "Accuracy of held-out model predictions against ground truth labels: 0.9464882943143813\n"
     ]
    }
   ],
   "source": [
    "## Code your solution here\n",
    "labels_from_majority_vote = []\n",
    "for i in range(len(multiannotator_labels)):\n",
    "    annotations_for_example_i = multiannotator_labels.iloc[i][pd.notna(multiannotator_labels.iloc[i])]\n",
    "    cnt_dict = {}\n",
    "    max_cnt = 0\n",
    "    for ann in annotations_for_example_i.values:\n",
    "        cnt_dict[ann] = cnt_dict.get(ann, 0) + 1\n",
    "        if cnt_dict[ann] > max_cnt:\n",
    "            max_cnt = cnt_dict[ann]\n",
    "    ann_list = list(cnt_dict.keys())\n",
    "    for ann in ann_list:\n",
    "        if cnt_dict[ann] < max_cnt:\n",
    "            del cnt_dict[ann]\n",
    "    majority_vote_label_example_i = list(cnt_dict.keys())[0]\n",
    "    labels_from_majority_vote.append(majority_vote_label_example_i)\n",
    "\n",
    "print(f\"Accuracy of majority vote annotators' labels against ground truth labels: {np.mean(labels_from_majority_vote == true_labels)}\")\n",
    "pred_probs_from_model_fit_to_majority_vote = train_model(labels_to_fit = labels_from_majority_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c58f05d-105d-46eb-b920-5dd6a3765e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst 10 annotators:\n",
      "A0048, 0.4062\n",
      "A0044, 0.4286\n",
      "A0046, 0.4828\n",
      "A0043, 0.4848\n",
      "A0045, 0.5000\n",
      "A0022, 0.5625\n",
      "A0041, 0.5714\n",
      "A0042, 0.5714\n",
      "A0050, 0.5750\n",
      "A0049, 0.5833\n"
     ]
    }
   ],
   "source": [
    "annotator_list = list(multiannotator_labels.columns)\n",
    "# number of correct labels given by annotator, where more than one annotator gave labels\n",
    "annotator_correct_cnts = {\n",
    "    annotator: 0\n",
    "    for annotator in annotator_list\n",
    "}\n",
    "# number of labels given by annotator, where more than one annotator gave labels\n",
    "annotator_num_labels = {\n",
    "    annotator: 0\n",
    "    for annotator in annotator_list\n",
    "}\n",
    "for i in range(len(multiannotator_labels)):\n",
    "    majority_vote_label_example_i = labels_from_majority_vote[i]\n",
    "    num_annotators_example_i = sum(pd.notna(multiannotator_labels.iloc[i]))\n",
    "    if num_annotators_example_i == 1:  # only consider examples where more than one annotator gave label\n",
    "        continue\n",
    "\n",
    "    for annotator in annotator_list:\n",
    "        if pd.isna(multiannotator_labels.iloc[i][annotator]):\n",
    "            continue\n",
    "        annotator_num_labels[annotator] += 1\n",
    "        if multiannotator_labels.iloc[i][annotator] == majority_vote_label_example_i:\n",
    "            annotator_correct_cnts[annotator] += 1\n",
    "\n",
    "annotator_scores = {\n",
    "    annotator: annotator_correct_cnts[annotator] / annotator_num_labels[annotator]\n",
    "    for annotator in annotator_list\n",
    "}\n",
    "annotator_score_tuples = list(annotator_scores.items())\n",
    "annotator_score_tuples = sorted(annotator_score_tuples, key=lambda x: x[1])\n",
    "\n",
    "print(\"Worst 10 annotators:\")\n",
    "for i in range(10):\n",
    "    print(f\"{annotator_score_tuples[i][0]}, {annotator_score_tuples[i][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a2e9d7-1d50-41e0-8441-9e7d7f4b9297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of majority vote annotators' labels after tie breaker against ground truth labels: 0.9130434782608695\n",
      "Accuracy of held-out model predictions against ground truth labels: 0.9565217391304348\n"
     ]
    }
   ],
   "source": [
    "# Break ties using the trained model\n",
    "labels_from_majority_vote_after_tie_breaker = []\n",
    "for i in range(len(multiannotator_labels)):\n",
    "    annotations_for_example_i = multiannotator_labels.iloc[i][pd.notna(multiannotator_labels.iloc[i])]\n",
    "    cnt_dict = {}\n",
    "    max_cnt = 0\n",
    "    for ann in annotations_for_example_i.values:\n",
    "        cnt_dict[ann] = cnt_dict.get(ann, 0) + 1\n",
    "        if cnt_dict[ann] > max_cnt:\n",
    "            max_cnt = cnt_dict[ann]\n",
    "    ann_list = list(cnt_dict.keys())\n",
    "    for ann in ann_list:\n",
    "        if cnt_dict[ann] < max_cnt:\n",
    "            del cnt_dict[ann]\n",
    "    if len(cnt_dict.keys()) > 1:\n",
    "        # Break ties using trained model\n",
    "        model_pred = pred_probs_from_model_fit_to_majority_vote[i, :].argmax()\n",
    "        if model_pred in cnt_dict.keys():\n",
    "            majority_vote_label_example_i = model_pred\n",
    "        else:\n",
    "            majority_vote_label_example_i = list(cnt_dict.keys())[0]\n",
    "    else:\n",
    "        majority_vote_label_example_i = list(cnt_dict.keys())[0]\n",
    "    labels_from_majority_vote_after_tie_breaker.append(majority_vote_label_example_i)\n",
    "\n",
    "print(\n",
    "    f\"Accuracy of majority vote annotators' labels after tie breaker against \"\n",
    "    + f\"ground truth labels: {np.mean(labels_from_majority_vote_after_tie_breaker == true_labels)}\"\n",
    ")\n",
    "pred_probs_from_model_fit_to_majority_vote = train_model(labels_to_fit = labels_from_majority_vote_after_tie_breaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffdd84fc-c84b-4d6f-a2b0-c4d279123409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of majority vote annotators' labels after tie breaker against ground truth labels: 0.8762541806020067\n",
      "Accuracy of held-out model predictions against ground truth labels: 0.9498327759197325\n"
     ]
    }
   ],
   "source": [
    "# Break ties using the annotator score\n",
    "annotator_list = list(multiannotator_labels.columns)\n",
    "\n",
    "labels_from_majority_vote_after_tie_breaker = []\n",
    "for i in range(len(multiannotator_labels)):\n",
    "    cnt_dict = {}\n",
    "    max_cnt = 0\n",
    "    for annotator in annotator_list:\n",
    "        if pd.isna(multiannotator_labels.iloc[i][annotator]):\n",
    "            continue\n",
    "        ann = multiannotator_labels.iloc[i][annotator]\n",
    "        if ann in cnt_dict.keys():\n",
    "            cnt_dict[ann]['result'] += 1\n",
    "            cnt_dict[ann]['annotator_scores'].append(annotator_scores[annotator])\n",
    "        else:\n",
    "            cnt_dict[ann] = {\n",
    "                'result': 1,\n",
    "                'annotator_scores': [annotator_scores[annotator]]\n",
    "            }\n",
    "        if cnt_dict[ann]['result'] > max_cnt:\n",
    "            max_cnt = cnt_dict[ann]['result']\n",
    "\n",
    "    ann_list = list(cnt_dict.keys())\n",
    "    for ann in ann_list:\n",
    "        if cnt_dict[ann]['result'] < max_cnt:\n",
    "            del cnt_dict[ann]\n",
    "\n",
    "    # Accumulate annotator scores\n",
    "    acc_function = np.min\n",
    "    for ann in cnt_dict.keys():\n",
    "        cnt_dict[ann]['acc_annotator_score'] = acc_function(cnt_dict[ann]['annotator_scores'])\n",
    "    cnt_dict_items = cnt_dict.items()\n",
    "    cnt_dict_items_sorted = sorted(cnt_dict_items, key=lambda x: x[1]['acc_annotator_score'])\n",
    "    majority_vote_label_example_i = cnt_dict_items_sorted[-1][0]\n",
    "\n",
    "    labels_from_majority_vote_after_tie_breaker.append(majority_vote_label_example_i)\n",
    "\n",
    "print(\n",
    "    f\"Accuracy of majority vote annotators' labels after tie breaker against \"\n",
    "    + f\"ground truth labels: {np.mean(labels_from_majority_vote_after_tie_breaker == true_labels)}\"\n",
    ")\n",
    "pred_probs_from_model_fit_to_majority_vote = train_model(labels_to_fit = labels_from_majority_vote_after_tie_breaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38fb1a9",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Estimate consensus labels for each example from the data in `multiannotator_labels`, this time using the CROWDLAB algorithm. You may find it helpful to reference: https://docs.cleanlab.ai/stable/tutorials/multiannotator.html\n",
    "Recall that CROWDLAB requires out of sample predicted class probabilities from a trained classifier. You may use the `pred_probs` from your model trained on majority-vote consensus labels or our randomly-selected annotator labels. Which do you think would be better to use?\n",
    "\n",
    "- Evaluate the accuracy of these CROWDLAB consensus labels against the ground truth labels.\n",
    "- Also set these as `labels_to_fit` in `train_model()` to see the resulting model's accuracy when trained with CROWDLAB consensus labels.\n",
    "- Estimate the quality of annotator (how accurate their labels tend to be overall) using CROWDLAB (assume the ground truth labels are unavailable as they would be in practice). Who do you guess are the worst 10 annotators based on this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ae8771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adbhut/Desktop/study_material/dcai-lab/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Code your solution here\n",
    "from cleanlab.multiannotator import get_label_quality_multiannotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12cd9539-a08e-45a5-8d69-dfbcb2afd9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-27 16:36:53.326606: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-27 16:36:53.989523: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-27 16:36:53.989618: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-27 16:36:53.989746: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-27 16:36:54.523582: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-27 16:36:54.566146: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-27 16:37:02.504399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "cleanlab_results = get_label_quality_multiannotator(\n",
    "    multiannotator_labels,\n",
    "    pred_probs_from_model_fit_to_majority_vote\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f020d89-051b-45b8-a3e1-99dbf18de6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of cleanlab labels against ground truth labels: 0.9765886287625418\n",
      "Accuracy of held-out model predictions against ground truth labels: 0.9732441471571907\n"
     ]
    }
   ],
   "source": [
    "labels_from_cleanlab = cleanlab_results['label_quality']['consensus_label'].values\n",
    "\n",
    "print(f\"Accuracy of cleanlab labels against ground truth labels: {np.mean(labels_from_cleanlab == true_labels)}\")\n",
    "pred_probs_from_model_fit_to_cleanlab_labels = train_model(labels_to_fit = labels_from_cleanlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "076891f3-5172-406e-adfc-5a71f94ba5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_quality</th>\n",
       "      <th>agreement_with_consensus</th>\n",
       "      <th>worst_class</th>\n",
       "      <th>num_examples_labeled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A0043</th>\n",
       "      <td>0.358109</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0048</th>\n",
       "      <td>0.374314</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0044</th>\n",
       "      <td>0.391327</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0042</th>\n",
       "      <td>0.396502</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0046</th>\n",
       "      <td>0.415364</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0041</th>\n",
       "      <td>0.431990</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0040</th>\n",
       "      <td>0.434583</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0045</th>\n",
       "      <td>0.467157</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0050</th>\n",
       "      <td>0.469913</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0049</th>\n",
       "      <td>0.469948</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       annotator_quality  agreement_with_consensus  worst_class  \\\n",
       "A0043           0.358109                  0.363636            2   \n",
       "A0048           0.374314                  0.406250            1   \n",
       "A0044           0.391327                  0.428571            2   \n",
       "A0042           0.396502                  0.428571            1   \n",
       "A0046           0.415364                  0.448276            1   \n",
       "A0041           0.431990                  0.464286            1   \n",
       "A0040           0.434583                  0.481481            2   \n",
       "A0045           0.467157                  0.461538            2   \n",
       "A0050           0.469913                  0.525000            2   \n",
       "A0049           0.469948                  0.527778            2   \n",
       "\n",
       "       num_examples_labeled  \n",
       "A0043                    33  \n",
       "A0048                    32  \n",
       "A0044                    21  \n",
       "A0042                    35  \n",
       "A0046                    29  \n",
       "A0041                    28  \n",
       "A0040                    27  \n",
       "A0045                    26  \n",
       "A0050                    40  \n",
       "A0049                    36  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanlab_results['annotator_stats'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d49be28-eced-4cdb-a880-1f7ec6ddd4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff69946-89bc-4853-a2b4-c74563bbaa9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
